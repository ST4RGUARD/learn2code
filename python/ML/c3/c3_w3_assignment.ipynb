{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Deep Q-Learning Lunar Lander",
   "id": "360acfbbc048477a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:31:16.612247Z",
     "start_time": "2025-05-26T15:31:16.589689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils3\n",
    "from PIL import Image\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "id": "5ae7c27e01402362",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:31:21.637257Z",
     "start_time": "2025-05-26T15:31:21.633702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instead of using Display, we can set the rendering mode directly in the environment\n",
    "# when creating it. Remove this line:\n",
    "# Display(visible=0, size=(840, 480)).start()\n",
    "\n",
    "# Set the random seed for TensorFlow\n",
    "tf.random.set_seed(utils3.SEED)"
   ],
   "id": "fd03fdea1bc58155",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:31:23.498721Z",
     "start_time": "2025-05-26T15:31:23.496332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate\n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ],
   "id": "c2cf0d959435c9c4",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lunar Lander Environment\n",
    "\n",
    "Load Env"
   ],
   "id": "84770daaa1e5838e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:31:41.174375Z",
     "start_time": "2025-05-26T15:31:41.163353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# When creating the environment, you can specify render_mode='rgb_array' \n",
    "# or disable rendering if you don't need visualization\n",
    "# Create environment with specified render mode\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "\n",
    "# Reset the environment\n",
    "observation = env.reset()[0]  # [0] because reset now returns (observation, info)\n",
    "\n",
    "# Render\n",
    "frame = env.render()  # No mode parameter needed here\n",
    "image = Image.fromarray(frame)"
   ],
   "id": "c2f1d5dc8d498778",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:31:56.496618Z",
     "start_time": "2025-05-26T15:31:56.494276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ],
   "id": "7eb65ad336d4813b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (8,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:32:10.162080Z",
     "start_time": "2025-05-26T15:32:10.159187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reset the environment and get the initial state.\n",
    "current_state = env.reset()"
   ],
   "id": "17c278b4f7152fff",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "# Display table with values.\n",
    "utils3.display_table(current_state, action, next_state, reward, done)\n",
    "\n",
    "# Replace the `current_state` with the state after the action is taken\n",
    "current_state = next_state"
   ],
   "id": "748226146ffc988d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exercise 1",
   "id": "1835861c5fa9d52e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:39:28.790254Z",
     "start_time": "2025-05-26T15:39:28.768749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# UNQ_C1\n",
    "# GRADED CELL\n",
    "\n",
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    ### START CODE HERE ###\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_actions, activation='linear'),\n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    ])\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_actions, activation='linear'),\n",
    "\n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    ])\n",
    "\n",
    "### START CODE HERE ###\n",
    "optimizer = Adam(learning_rate=ALPHA)\n",
    "### END CODE HERE ###"
   ],
   "id": "eae74f2c8d047f0e",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# UNIT TEST\n",
    "from public_tests5 import *\n",
    "\n",
    "test_network(q_network)\n",
    "test_network(target_q_network)\n",
    "test_optimizer(optimizer, ALPHA)"
   ],
   "id": "b5cf8d2b130f6ddf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:39:39.892363Z",
     "start_time": "2025-05-26T15:39:39.870409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import relu, linear\n",
    "\n",
    "def create_q_network():\n",
    "    state_size = 8  # From the test requirements\n",
    "    num_actions = 4  # From the test requirements\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(state_size,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_actions, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the networks\n",
    "q_network = create_q_network()\n",
    "target_q_network = create_q_network()\n",
    "\n",
    "# Define optimizer"
   ],
   "id": "3cc91b892e7a48c4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jarjohns/ML/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exercise 2",
   "id": "5c23b7fb621688b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:41:11.475386Z",
     "start_time": "2025-05-26T15:41:11.470227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: calculate_loss\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\"\n",
    "    Calculates the loss.\n",
    "\n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "\n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "\n",
    "    # Set y = R if episode terminates, otherwise set y = R + Î³ max Q^(s,a).\n",
    "    ### START CODE HERE ###\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "\n",
    "    # Compute the loss\n",
    "    ### START CODE HERE ###\n",
    "    loss = MSE(y_targets, q_values)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return loss"
   ],
   "id": "6b4506c3a88ecc59",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:41:19.528763Z",
     "start_time": "2025-05-26T15:41:19.497045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# UNIT TEST\n",
    "test_compute_loss(compute_loss)"
   ],
   "id": "9c1d7de5b0cce706",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mAll tests passed!\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
